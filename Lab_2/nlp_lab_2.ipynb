{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":406,"status":"ok","timestamp":1693653795221,"user":{"displayName":"deleted","userId":"14503722011509702990"},"user_tz":-120},"id":"00Dts_nAzdhW","outputId":"81b2e14a-2055-4ba0-d0c7-5898d292e59b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\damio\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.util import ngrams\n","import string\n","import numpy as np\n","from sklearn.utils import shuffle\n","import collections\n","import re\n","import math"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def tokenize(str_data):\n","    data = str_data.lower()\n","    data = re.sub(r'\\n', '', data)\n","    data = re.sub(r'[^A-Za-z\\s]', '', data)\n","    data = data.split(' ')\n","    data = list(filter(lambda item: item != '', data))\n","    return data\n","\n","def extract_ngrams(tokens, n = 2):\n","    ngram_list = ngrams(tokens, n)\n","    return list(ngram_list)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["books = []\n","labels = [1, 2, 3, 4, 5, 6, 7]\n","for book in labels:\n","    with open (f\"harry_potter/HP{book}.txt\", 'r') as f:\n","        data = f.read()\n","        books.append(extract_ngrams(tokenize(data), 2))\n","        f.close()"]},{"cell_type":"markdown","metadata":{},"source":["#### Create the dataset, shuffle and split into train, test, validate"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#Merge all the ngrams together and have corresponding labels\n","data = []\n","labels = []\n","for k in range(len(books)):\n","    data += books[k]\n","    labels += [k+1 for i in range(len(books[k]))]\n","\n","books = None\n","data = np.array(data)\n","labels = np.array(labels)\n","\n","data, labels = shuffle(data, labels, random_state=0)\n","\n","#Split the data into training and testing and validation\n","train_data = data[:int(len(data)*0.8)]\n","train_labels = labels[:int(len(labels)*0.8)]\n","\n","test_data = data[int(len(data)*0.8):int(len(data)*0.9)]\n","test_labels = labels[int(len(labels)*0.8):int(len(labels)*0.9)]\n","\n","val_data = data[int(len(data)*0.9):]\n","val_labels = labels[int(len(labels)*0.9):]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(872433, 2) (109054, 2) (109055, 2)\n","(872433,) (109054,) (109055,)\n"]}],"source":["print(train_data.shape, test_data.shape, val_data.shape)\n","print(train_labels.shape, test_labels.shape, val_labels.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["#Get the word frequencies for the training set \n","word_frequencies = collections.defaultdict(lambda: collections.defaultdict(int))\n","for label, ngram in zip(train_labels, train_data):\n","    word_frequencies[label][tuple(ngram)] += 1"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class_counts = collections.Counter(label for label in train_labels)\n","class_priors = {cls: count / len(train_data) for cls, count in class_counts.items()}"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def classify(data, delta = 0.1):\n","    probabilities = {}\n","    for label in class_counts.keys():\n","        prob = math.log(class_priors[label])\n","        # for ngram in data:\n","        n_count = word_frequencies[label][tuple(data)] + delta\n","        n_total = sum(word_frequencies[label].values()) + delta * len(word_frequencies[label])\n","        prob += math.log(n_count / n_total)\n","        probabilities[label] = prob\n","    return probabilities"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["sentence = \"But there was still a fortnight to go before he went back to school. He looked hopelessly around his room again, and his eye paused on the birthday cards his two best friends had sent him at the end of July. What would they say if Harry wrote to them and told them about his scar hurting? At once, Hermione Granger’s voice seemed to fill his head, shrill and panicky. “ Your scar hurt? Harry, that’s really serious. . . . Write to Professor Dumbledorel And I’ll go and check Common Magical Ailments and Afflictions. ... Maybe there’s something in there about curse scars. ...” Yes, that would be Hermione’s advice: Go straight to the headmaster of Hogwarts, and in the meantime, consult a book. Harry stared out of the window at the inky blue-black sky. He doubted very much whether a book could help him now. As far as he knew, he was the only living person to have survived a curse like Voldemort’s; it was highly unlikely, therefore, that he would find his symptoms listed in Common Magical Ailments and Afflictions. As for informing the headmaster, Harry had no idea where Dumbledore went during the summer holidays. He amused himself for a moment, picturing Dumbledore, with his long silver beard, full-length wizard’s robes, and pointed hat, stretched out on a beach somewhere, rubbing suntan lotion onto his long crooked nose. Wherever Dumbledore was, though, Harry was sure that Hedwig would be able to find him; Harry’s owl had never yet failed to deliver a letter to anyone, even without an address. But what would he write? Dear Professor Dumbledore, Sorry to bother you, but my scar hurt this morning. Yours sincerely, Harry Potter.\"\n","sentence = extract_ngrams(tokenize(sentence), 2)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.25867001668897976\n"]}],"source":["accuracy = 0\n","tot = 0\n","for dp in zip(test_data, test_labels):\n","    probs = classify(dp[0])\n","    if max(probs, key=probs.get) == dp[1]:\n","        accuracy += 1\n","    \n","    tot += 1\n","\n","print(accuracy / tot)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["array(['pain', 'seared'], dtype='<U41')"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["test_data[0]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["pain\n","seared\n"]}],"source":["for d in test_data[0]:\n","    print(d)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMXZxYGcJXUwKbD7EGoorUO","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
